# -*- coding: utf-8 -*-
import json
import re
import sys
import requests
from pyquery import PyQuery as pq
from urllib.parse import urlparse, urljoin
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

sys.path.append('..')
from base.spider import Spider

class Spider(Spider):
    name = "é¦™è•‰è³‡æº"
    base_url = "https://25kkuu.vip/xjzy"
    version = "2024.07.15"

    def init(self, extend=""):
        self.session = requests.Session()
        self.retries = Retry(total=3, backoff_factor=1, 
                           status_forcelist=[500, 502, 503, 504])
        self.session.mount('https://', HTTPAdapter(max_retries=self.retries))
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',
            'Referer': self.base_url + '/',
            'X-Requested-With': 'XMLHttpRequest'
        }

        try:
            if extend:
                extend_data = json.loads(extend)
                self.base_url = extend_data.get('host', self.base_url).rstrip('/')
                self.session.proxies = extend_data.get('proxies', {})
        except Exception as e:
            print(f"æ“´å±•åƒæ•¸è§£æå¤±æ•—: {e}")

    # ----------- é—œéµä¿®è¦†éƒ¨åˆ† -----------
    def categoryContent(self, tid, pg, filter, extend):
        try:
            # ä¿®æ­£åˆ†é¡é URLæ¨¡å¼
            url = f"{self.base_url}/vod/show/{tid}/--------{pg}---.html"
            print(f"åˆ†é¡é URL: {url}")  # èª¿è©¦æ—¥å¿—
            html = self._fetchUrl(url)
            doc = pq(html)
            return {
                'list': self._parseVideoList(doc('.list-videos .item')),
                'page': int(pg),
                'pagecount': 9999,
                'limit': 30,
                'total': 999999
            }
        except Exception as e:
            print(f"åˆ†é¡é ç•°å¸¸ [{tid}-{pg}]: {str(e)}")
            return {'list': []}

    def _parseVideoList(self, items):
        videos = []
        for item in items.items():
            try:
                href = item('a').attr('href')
                print(f"åŸå§‹href: {href}")  # èª¿è©¦æ—¥å¿—
                
                # å¢å¼·IDæå–
                vid = None
                patterns = [
                    r'xplay(\d+)\.html',
                    r'-(\d+)\.html',
                    r'/video/(\d+)/'
                ]
                for pattern in patterns:
                    if match := re.search(pattern, href):
                        vid = match.group(1)
                        break
                
                if not vid:
                    print(f"ç„¡æ³•è§£æè¦–é »ID: {href}")
                    continue
                
                # å°é¢è™•ç†
                img = item('img')
                cover = self._normalizeUrl(
                    img.attr('data-src') or 
                    img.attr('src') or 
                    img.attr('data-original')
                )
                
                videos.append({
                    'vod_id': vid,
                    'vod_name': item('.title').text().strip(),
                    'vod_pic': cover,
                    'vod_remarks': item('.duration').text()
                })
            except Exception as e:
                print(f"åˆ—è¡¨é …è§£æå¤±æ•—: {str(e)}")
        return videos

    def _parsePlayerUrl(self, html):
        """å¢å¼·ç‰ˆæ’­æ”¾åœ°å€è§£æ"""
        # æ¨¡å¼1ï¼šJSONæ•¸æ“šå¡Š
        if match := re.search(r'var playerData = ({.*?});', html, re.DOTALL):
            try:
                data = json.loads(match.group(1))
                return data.get('url', '')
            except json.JSONDecodeError:
                pass
        
        # æ¨¡å¼2ï¼šiframeåµŒå¥—
        if iframe_src := re.search(r'<iframe[^>]+src="(https?://[^"]+)"', html):
            print(f"ç™¼ç¾iframe: {iframe_src.group(1)}")
            iframe_html = self._fetchUrl(iframe_src.group(1))
            return self._parsePlayerUrl(iframe_html)  # éæ­¸è§£æ
        
        # æ¨¡å¼3ï¼šæ¨™æº–m3u8åŒ¹é…
        if m3u8 := re.search(r'(https?://[^\s"\'<>]+?\.m3u8)', html):
            return m3u8.group(0)
        
        return ''

    # ----------------------------------
    # å…¶ä½™ä¿æŒåŸæœ‰ä»£ç¢¼çµæ§‹ï¼ˆhomeContent/detailContentç­‰ï¼‰
    # ...

    def _fetchUrl(self, url, retry=2):
        """å¸¶èª¿è©¦çš„è«‹æ±‚æ–¹æ³•"""
        print(f"ğŸŒ€ è«‹æ±‚: {url}")
        try:
            resp = self.session.get(url, headers=self.headers, timeout=15)
            print(f"éŸ¿æ‡‰ç‹€æ…‹: {resp.status_code}")
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            print(f"âŒ è«‹æ±‚å¤±æ•— [{url}]: {str(e)}")
            if retry > 0:
                return self._fetchUrl(url, retry-1)
            return ""

if __name__ == '__main__':
    # æœ¬åœ°æ¸¬è©¦ä»£ç¢¼
    spider = Spider()
    spider.init()
    
    # æ¸¬è©¦é¦–é 
    print("æ¸¬è©¦é¦–é :", spider.homeContent({}))
    
    # æ¸¬è©¦åˆ†é¡é 
    print("æ¸¬è©¦åˆ†é¡:", spider.categoryContent("cn-jieshuoyuanpian", 1, {}, {}))
    
    # æ¸¬è©¦è©³æƒ…é 
    print("æ¸¬è©¦è©³æƒ…:", spider.detailContent(["123"]))  # æ›¿æ›å¯¦éš›ID
