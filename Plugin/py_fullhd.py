import requests
from bs4 import BeautifulSoup
import re
import sys
import json
import base64
import urllib.parse
from Crypto.Cipher import ARC4
from Crypto.Util.Padding import unpad
import binascii
from urllib.parse import urljoin
import warnings
from base.spider import Spider  # å‡è¨­é€™æ˜¯æ¡†æ¶åŸºé¡

# ç¦ç”¨SSLè­¦å‘Šï¼ˆåƒ…æ¸¬è©¦ç’°å¢ƒä½¿ç”¨ï¼‰
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

# å…¨å±€é…ç½®
xurl = "https://www.fullhd.xxx/zh/"
headerx = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': xurl  # é—œéµé˜²ç›œéˆè¨­ç½®
}
pm = ''

class FullHDSpider(Spider):
    def __init__(self):
        super().__init__()
        self.xurl = xurl
        self.headerx = headerx

    def getName(self):
        return "FullHDèœ˜è››"

    # ä»¥ä¸‹ç‚ºæ¡†æ¶è¦æ±‚å¿…é ˆå¯¦ç¾çš„æ–¹æ³•
    def init(self, extend):
        pass

    def isVideoFormat(self, url):
        pass

    def manualVideoCheck(self):
        pass

    # æ ¸å¿ƒæ–¹æ³•ï¼šæå–ä¸­é–“æ–‡æœ¬
    def extract_middle_text(self, text, start_str, end_str, pl, start_index1='', end_index2=''):
        try:
            if pl == 3:
                plx = []
                while True:
                    start_index = text.find(start_str)
                    if start_index == -1:
                        break
                    end_index = text.find(end_str, start_index + len(start_str))
                    if end_index == -1:
                        break
                    middle_text = text[start_index + len(start_str):end_index]
                    plx.append(middle_text)
                    text = text.replace(start_str + middle_text + end_str, '')
                if plx:
                    purl = ''
                    for item in plx:
                        matches = re.findall(start_index1, item)
                        output = ""
                        for match in matches:
                            match3 = re.search(r'(?:^|[^0-9])(\d+)(?:[^0-9]|$)', match[1])
                            number = match3.group(1) if match3 else '0'
                            url_part = urljoin(self.xurl, match[0]) if 'http' not in match[0] else match[0]
                            output += f"#{'ğŸ“½ï¸' + match[1]}${number}{url_part}"
                        purl += output[1:] + "$$$"
                    return purl[:-3]
                return ""
            else:
                start_index = text.find(start_str)
                if start_index == -1:
                    return ""
                end_index = text.find(end_str, start_index + len(start_str))
                if end_index == -1:
                    return ""
                middle_text = text[start_index + len(start_str):end_index]
                if pl == 0:
                    return middle_text.replace("\\", "")
                elif pl == 1:
                    matches = re.findall(start_index1, middle_text)
                    return ' '.join(matches) if matches else ''
                elif pl == 2:
                    matches = re.findall(start_index1, middle_text)
                    return '$$$'.join([f'âœ¨{item}' for item in matches]) if matches else ''
        except Exception as e:
            print(f"[extract_middle_text ERROR] {str(e)}")
            return ""

    # ä¸»é åˆ†é¡
    def homeContent(self, filter):
        return {
            "class": [
                {"type_id": "latest-updates", "type_name": "æœ€æ–°è§†é¢‘ğŸŒ "},
                {"type_id": "top-rated", "type_name": "æœ€ä½³è§†é¢‘ğŸŒ "},
                {"type_id": "most-popular", "type_name": "çƒ­é—¨å½±ç‰‡ğŸŒ "}
            ]
        }

    # ä¸»é è¦–é »åˆ—è¡¨
    def homeVideoContent(self):
        try:
            response = requests.get(self.xurl, headers=self.headerx, verify=False, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            return self._parse_video_list(soup)
        except Exception as e:
            print(f"[homeVideoContent ERROR] {str(e)}")
            return {'list': []}

    # åˆ†é¡é é¢
    def categoryContent(self, cid, pg, filter, ext):
        try:
            page = int(pg) if pg else 1
            path = f"/{cid}/{page}/" if page > 1 else f"/{cid}/"
            url = urljoin(self.xurl, path)
            response = requests.get(url, headers=self.headerx, verify=False, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            result = self._parse_video_list(soup)
            result.update({
                'page': page,
                'pagecount': 9999,
                'limit': 90,
                'total': 999999
            })
            return result
        except Exception as e:
            print(f"[categoryContent ERROR] {str(e)}")
            return {'list': []}

    # è¦–é »è©³æƒ…
    def detailContent(self, ids):
        try:
            did = ids[0]
            if not did.startswith('http'):
                did = urljoin(self.xurl, did)
            response = requests.get(did, headers=self.headerx, verify=False, timeout=10)
            response.raise_for_status()
            
            content = self.extract_middle_text(response.text, '<h1>', '</h1>', 0)
            yanuan = self.extract_middle_text(
                response.text, 
                '<span>Pornstars:</span>', 
                '</div>', 
                1, 
                'href=".*?">(.*?)</a>'
            )
            
            return {
                'list': [{
                    "vod_id": did,
                    "vod_actor": yanuan,
                    "vod_content": content,
                    "vod_play_from": 'ç·šè·¯ä¸€',
                    "vod_play_url": did
                }]
            }
        except Exception as e:
            print(f"[detailContent ERROR] {str(e)}")
            return {'list': []}

    # æ’­æ”¾è§£æ
    def playerContent(self, flag, id, vipFlags):
        try:
            if 'http' not in id:
                id = urljoin(self.xurl, id)
            
            response = requests.get(id, headers=self.headerx, verify=False, timeout=10)
            response.raise_for_status()
            
            video_tag = BeautifulSoup(response.text, "lxml").find('video')
            if not video_tag:
                raise ValueError("Video tag not found")
            
            source = video_tag.find('source')
            if not source:
                raise ValueError("Source tag not found")
                
            video_url = source.get('src')
            if not video_url:
                raise ValueError("Video URL not found")
            
            # è™•ç†é‡å®šå‘
            for _ in range(2):  # æœ€å¤šå…©æ¬¡é‡å®šå‘
                resp = requests.head(video_url, allow_redirects=False, timeout=10)
                if 300 <= resp.status_code < 400:
                    video_url = resp.headers['Location']
                else:
                    break
            
            return {
                "parse": 0,
                "playUrl": "",
                "url": video_url,
                "header": json.dumps(self.headerx)  # åºåˆ—åŒ–é ­éƒ¨
            }
        except Exception as e:
            print(f"[playerContent ERROR] {str(e)}")
            return {}

    # æœç´¢åŠŸèƒ½
    def searchContent(self, key, quick):
        return self.searchContentPage(key, quick, '1')

    def searchContentPage(self, key, quick, page):
        try:
            page = int(page) if page else 1
            path = f"/search/{key}/{page}/" if page > 1 else f"/search/{key}/"
            url = urljoin(self.xurl, path)
            response = requests.get(url, headers=self.headerx, verify=False, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")
            result = self._parse_video_list(soup)
            result.update({
                'page': page,
                'pagecount': 9999,
                'limit': 90,
                'total': 999999
            })
            return result
        except Exception as e:
            print(f"[searchContentPage ERROR] {str(e)}")
            return {'list': []}

    # ç§æœ‰å·¥å…·æ–¹æ³•
    def _parse_video_list(self, soup):
        """çµ±ä¸€è§£æè¦–é »åˆ—è¡¨"""
        videos = []
        container = soup.find('div', class_="margin-fix")
        if not container:
            return {'list': []}
            
        for item in container.find_all('div', class_="item"):
            try:
                # åç¨±èˆ‡éˆæ¥
                link_tag = item.find('a')
                name = link_tag.get('title', 'æœªçŸ¥æ¨™é¡Œ')
                href = link_tag.get('href', '#')
                
                # åœ–ç‰‡è™•ç†
                img_tag = item.find('img', class_="lazyload")
                pic = self._process_image(img_tag)
                
                # æ™‚é•·æ¨™è¨»
                remark_tag = item.find('div', class_="img thumb__img")
                remark = remark_tag.text.strip() if remark_tag else ''
                
                videos.append({
                    "vod_id": href,
                    "vod_name": name,
                    "vod_pic": pic,
                    "vod_remarks": remark
                })
            except Exception as e:
                print(f"[_parse_video_list ITEM ERROR] {str(e)}")
                continue
                
        return {'list': videos}

    def _process_image(self, img_tag):
        """çµ±ä¸€è™•ç†åœ–ç‰‡æ¨™ç±¤"""
        if not img_tag:
            return ''
        attrs_priority = ['data-src', 'data-original', 'src']
        for attr in attrs_priority:
            if img_tag.has_attr(attr):
                return urljoin(self.xurl, img_tag[attr])
        return ''

    # ä»£ç†æ–¹æ³•ï¼ˆæ ¹æ“šæ¡†æ¶è¦æ±‚å¯¦ç¾ï¼‰
    def localProxy(self, params):
        if params['type'] == "m3u8":
            return self.proxyM3u8(params)
        elif params['type'] == "media":
            return self.proxyMedia(params)
        elif params['type'] == "ts":
            return self.proxyTs(params)
        return None

# æ¸¬è©¦å…¥å£
if __name__ == '__main__':
    spider = FullHDSpider()
    
    # æ¸¬è©¦ä¸»é 
    print("=== ä¸»é åˆ†é¡ ===")
    print(spider.homeContent({}))
    
    # æ¸¬è©¦ä¸»é è¦–é »
    print("\n=== ä¸»é è¦–é » ===")
    print(json.dumps(spider.homeVideoContent(), ensure_ascii=False, indent=2))
    
    # æ¸¬è©¦åˆ†é¡
    print("\n=== åˆ†é¡å…§å®¹ ===")
    print(json.dumps(spider.categoryContent("latest-updates", 1, {}, {}), ensure_ascii=False, indent=2))
    
    # æ¸¬è©¦æœç´¢
    print("\n=== æœç´¢çµæœ ===")
    print(json.dumps(spider.searchContent("test", False), ensure_ascii=False, indent=2))
